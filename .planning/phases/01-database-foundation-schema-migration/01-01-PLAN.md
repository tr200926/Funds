---
phase: 01-database-foundation-schema-migration
plan: 01
name: "Database Foundation & Schema Migration"
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/20260212000001_create_core_schema.sql
  - supabase/migrations/20260212000002_create_rls_policies.sql
  - supabase/migrations/20260212000003_create_triggers.sql
  - supabase/migrations/20260212000004_seed_initial_data.sql
  - scripts/migrate_legacy_data.ts
  - lib/database.types.ts
autonomous: true

must_haves:
  truths:
    - "New normalized schema exists with 10 tables (organizations, profiles, platforms, ad_accounts, spend_records, balance_snapshots, alert_rules, alerts, alert_deliveries, notification_channels, pipeline_runs)"
    - "RLS policies enforce org-scoped data access for all user-facing tables"
    - "Database triggers update ad_accounts.current_* fields when new spend/balance data arrives"
    - "Legacy data from 7 old tables is migrated to new schema without data loss"
    - "Supabase Auth roles (admin, manager, viewer) can be assigned to users"
    - "TypeScript types for new schema are generated and available"
  artifacts:
    - path: "supabase/migrations/20260212000001_create_core_schema.sql"
      provides: "Creates 10 normalized tables with indexes and constraints"
      min_lines: 400
    - path: "supabase/migrations/20260212000002_create_rls_policies.sql"
      provides: "RLS policies for org-scoped access control"
      min_lines: 150
    - path: "supabase/migrations/20260212000003_create_triggers.sql"
      provides: "Database triggers for denormalized current_* updates"
      min_lines: 80
    - path: "scripts/migrate_legacy_data.ts"
      provides: "Data migration script from old tables to new schema"
      min_lines: 200
    - path: "lib/database.types.ts"
      provides: "Generated TypeScript types for Supabase"
      exports: ["Database", "Tables"]
  key_links:
    - from: "spend_records table"
      to: "ad_accounts.current_daily_spend"
      via: "on_spend_record_insert trigger"
      pattern: "UPDATE ad_accounts SET current_daily_spend"
    - from: "balance_snapshots table"
      to: "ad_accounts.current_balance"
      via: "on_balance_snapshot_insert trigger"
      pattern: "UPDATE ad_accounts SET current_balance"
    - from: "profiles table"
      to: "auth.users table"
      via: "foreign key on id column"
      pattern: "REFERENCES auth.users\\(id\\)"
    - from: "scripts/migrate_legacy_data.ts"
      to: "ad_accounts, spend_records, balance_snapshots tables"
      via: "INSERT statements from legacy data"
      pattern: "INSERT INTO ad_accounts|spend_records|balance_snapshots"
---

# Plan 01: Database Foundation & Schema Migration

## Objective

Create the normalized Supabase database schema that serves as the foundation for the entire platform. This includes 10 core tables with proper relationships, Row Level Security (RLS) policies for multi-tenant readiness, database triggers for denormalized performance fields, and a zero-downtime migration script to backfill data from the 7 existing legacy tables.

**Purpose:** Every other component (n8n pipelines, dashboard, alert engine) depends on this normalized schema. Without it, no further work can proceed. This phase establishes the single source of truth for all ad spend data.

**Output:**
- Versioned Supabase migrations (SQL files)
- RLS policies enforcing org-scoped access
- Database triggers updating denormalized current_* fields
- Migration script preserving all historical data
- Generated TypeScript types for type-safe database access

## Execution Context

@c:\Users\hossa\.claude\get-shit-done\workflows\execute-plan.md
@c:\Users\hossa\.claude\get-shit-done\templates\summary.md

## Context

@c:\Users\hossa\Desktop\Funds project\.planning\REQUIREMENTS.md
@c:\Users\hossa\Desktop\Funds project\.planning\research\SUMMARY.md
@c:\Users\hossa\Desktop\Funds project\.planning\research\architecture.md
@c:\Users\hossa\Desktop\Funds project\database\schema.sql

## Tasks

<task type="auto">
  <name>Task 1: Initialize Supabase CLI and create core schema migration</name>

  <files>
  supabase/migrations/20260212000001_create_core_schema.sql
  supabase/config.toml
  .gitignore
  </files>

  <action>
  Initialize Supabase project structure and create the first migration file with all 10 core tables.

  **Step 1: Initialize Supabase CLI**
  ```bash
  # Install Supabase CLI if not present
  npm install -g supabase

  # Initialize Supabase project (links to existing project)
  supabase init

  # Update .gitignore to exclude Supabase CLI artifacts
  echo "supabase/.branches" >> .gitignore
  echo "supabase/.temp" >> .gitignore
  ```

  **Step 2: Create core schema migration**
  Create `supabase/migrations/20260212000001_create_core_schema.sql` with the following tables:

  **1. organizations table**
  - `id` UUID primary key
  - `name` TEXT NOT NULL
  - `slug` TEXT NOT NULL UNIQUE (for URL routing)
  - `timezone` TEXT NOT NULL DEFAULT 'Africa/Cairo'
  - `settings` JSONB NOT NULL DEFAULT '{}'
  - `created_at`, `updated_at`, `archived_at` TIMESTAMPTZ

  **2. profiles table** (extends auth.users)
  - `id` UUID primary key REFERENCES auth.users(id) ON DELETE CASCADE
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `full_name` TEXT NOT NULL
  - `role` TEXT NOT NULL DEFAULT 'viewer' CHECK (role IN ('admin', 'manager', 'viewer'))
  - `avatar_url` TEXT
  - `settings` JSONB NOT NULL DEFAULT '{}'
  - `created_at`, `updated_at` TIMESTAMPTZ
  - INDEX on org_id

  **3. platforms table** (reference data)
  - `id` TEXT primary key ('facebook', 'tiktok')
  - `display_name` TEXT NOT NULL ('Facebook Ads', 'TikTok Ads')
  - `api_version` TEXT ('v23.0', 'v1.3')
  - `icon_url` TEXT
  - `is_active` BOOLEAN NOT NULL DEFAULT true
  - `config` JSONB NOT NULL DEFAULT '{}'

  **4. ad_accounts table**
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `platform_id` TEXT NOT NULL REFERENCES platforms(id)
  - `platform_account_id` TEXT NOT NULL (e.g., "act_123456" or "7378858...")
  - `account_name` TEXT NOT NULL
  - `business_manager` TEXT (e.g., "Main", "Pasant", "Xlerate")
  - `currency` TEXT NOT NULL DEFAULT 'EGP'
  - `status` TEXT NOT NULL DEFAULT 'active' CHECK (status IN ('active', 'paused', 'disabled', 'archived'))
  - Denormalized fields for fast dashboard reads:
    - `current_balance` NUMERIC(14,2)
    - `current_daily_spend` NUMERIC(14,2)
    - `current_mtd_spend` NUMERIC(14,2)
    - `last_synced_at` TIMESTAMPTZ
  - `assigned_to` UUID REFERENCES profiles(id)
  - `tags` TEXT[] DEFAULT '{}'
  - `metadata` JSONB NOT NULL DEFAULT '{}' (platform-specific fields)
  - `created_at`, `updated_at`, `archived_at` TIMESTAMPTZ
  - UNIQUE(org_id, platform_id, platform_account_id)
  - INDEX on org_id, platform_id, status (WHERE status = 'active'), assigned_to

  **5. spend_records table** (time-series, append-mostly)
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `ad_account_id` UUID NOT NULL REFERENCES ad_accounts(id)
  - `date` DATE NOT NULL (the spend date)
  - `daily_spend` NUMERIC(14,2) NOT NULL DEFAULT 0
  - `mtd_spend` NUMERIC(14,2) (month-to-date as of this date)
  - `currency` TEXT NOT NULL DEFAULT 'EGP'
  - `raw_data` JSONB (original API response for audit)
  - `pipeline_run_id` UUID
  - `created_at`, `updated_at` TIMESTAMPTZ
  - UNIQUE(ad_account_id, date)
  - INDEX on (org_id, date DESC), (ad_account_id, date DESC), (date DESC)

  **6. balance_snapshots table** (time-series, append-only)
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `ad_account_id` UUID NOT NULL REFERENCES ad_accounts(id)
  - `balance` NUMERIC(14,2) NOT NULL
  - `available_funds` TEXT (raw display string from API)
  - `currency` TEXT NOT NULL DEFAULT 'EGP'
  - `captured_at` TIMESTAMPTZ NOT NULL DEFAULT now()
  - `pipeline_run_id` UUID
  - `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()
  - INDEX on (ad_account_id, captured_at DESC), (org_id, captured_at DESC)

  **7. alert_rules table**
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `ad_account_id` UUID REFERENCES ad_accounts(id) (NULL = applies to all accounts)
  - `name` TEXT NOT NULL
  - `description` TEXT
  - `rule_type` TEXT NOT NULL CHECK (rule_type IN ('balance_threshold', 'spend_spike', 'time_to_depletion', 'spend_anomaly', 'account_status_change', 'zero_spend'))
  - `severity` TEXT NOT NULL DEFAULT 'warning' CHECK (severity IN ('info', 'warning', 'critical', 'emergency'))
  - `config` JSONB NOT NULL DEFAULT '{}' (rule parameters)
  - `cooldown_minutes` INT NOT NULL DEFAULT 180
  - `is_active` BOOLEAN NOT NULL DEFAULT true
  - `active_hours` JSONB (NULL = 24/7)
  - `created_at`, `updated_at` TIMESTAMPTZ
  - `created_by` UUID REFERENCES profiles(id)
  - INDEX on org_id, ad_account_id, (org_id WHERE is_active = true)

  **8. alerts table** (generated alert instances)
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `ad_account_id` UUID NOT NULL REFERENCES ad_accounts(id)
  - `alert_rule_id` UUID NOT NULL REFERENCES alert_rules(id)
  - `severity` TEXT NOT NULL
  - `title` TEXT NOT NULL
  - `message` TEXT NOT NULL
  - `context_data` JSONB NOT NULL DEFAULT '{}' (snapshot of triggering data)
  - `status` TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'acknowledged', 'resolved', 'dismissed'))
  - `acknowledged_at` TIMESTAMPTZ
  - `acknowledged_by` UUID REFERENCES profiles(id)
  - `resolved_at` TIMESTAMPTZ
  - `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()
  - INDEX on (org_id, created_at DESC), (ad_account_id, created_at DESC), (alert_rule_id, created_at DESC), (status WHERE status = 'pending')

  **9. alert_deliveries table** (per-channel delivery tracking)
  - `id` UUID primary key
  - `alert_id` UUID NOT NULL REFERENCES alerts(id) ON DELETE CASCADE
  - `channel_type` TEXT NOT NULL CHECK (channel_type IN ('email', 'telegram', 'whatsapp', 'webhook'))
  - `recipient` TEXT NOT NULL (email address, chat_id, phone number)
  - `status` TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'sent', 'failed', 'queued'))
  - `response_data` JSONB (API response)
  - `error_message` TEXT
  - `sent_at` TIMESTAMPTZ
  - `created_at` TIMESTAMPTZ NOT NULL DEFAULT now()
  - INDEX on (alert_id), (status WHERE status = 'failed')

  **10. notification_channels table** (channel configuration)
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `channel_type` TEXT NOT NULL
  - `name` TEXT NOT NULL
  - `config` JSONB NOT NULL DEFAULT '{}' (channel-specific settings like recipient, bot token, etc.)
  - `min_severity` TEXT NOT NULL DEFAULT 'warning'
  - `is_enabled` BOOLEAN NOT NULL DEFAULT true
  - `active_hours` JSONB (quiet hours)
  - `created_at`, `updated_at` TIMESTAMPTZ
  - INDEX on (org_id, is_enabled)

  **11. pipeline_runs table** (preserves STATUS WORKFLOWS pattern)
  - `id` UUID primary key
  - `org_id` UUID NOT NULL REFERENCES organizations(id)
  - `pipeline_name` TEXT NOT NULL ('facebook_ingestion', 'tiktok_ingestion')
  - `status` TEXT NOT NULL CHECK (status IN ('running', 'success', 'failed', 'partial'))
  - `started_at` TIMESTAMPTZ NOT NULL DEFAULT now()
  - `completed_at` TIMESTAMPTZ
  - `accounts_processed` INT DEFAULT 0
  - `accounts_failed` INT DEFAULT 0
  - `error_log` JSONB (errors by account)
  - `metadata` JSONB NOT NULL DEFAULT '{}'
  - INDEX on (org_id, started_at DESC), (pipeline_name, started_at DESC)

  **IMPORTANT NOTES:**
  - All TIMESTAMPTZ fields store in UTC (database default)
  - Use gen_random_uuid() for UUID primary keys
  - Include updated_at trigger for tables with updated_at column
  - Use NUMERIC(14,2) for all currency amounts (supports up to 999,999,999,999.99)
  - Foreign keys have ON DELETE CASCADE only where appropriate (profiles -> auth.users)
  - Indexes are focused on common query patterns (org_id + time DESC for dashboards)
  </action>

  <verify>
  ```bash
  # Verify migration file exists
  ls supabase/migrations/20260212000001_create_core_schema.sql

  # Verify SQL syntax (dry-run)
  supabase db reset --dry-run

  # Check that all 10 tables are defined
  grep -c "CREATE TABLE" supabase/migrations/20260212000001_create_core_schema.sql
  # Should output: 11 (10 tables + 1 for updated_at trigger function)

  # Verify critical indexes exist
  grep -c "CREATE INDEX" supabase/migrations/20260212000001_create_core_schema.sql
  # Should output: 15+
  ```
  </verify>

  <done>
  - supabase/migrations/20260212000001_create_core_schema.sql exists with all 10 table definitions
  - SQL syntax is valid (dry-run passes)
  - All foreign key relationships defined correctly
  - Indexes created for common query patterns
  - CHECK constraints on enum-like columns (role, status, severity, etc.)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Row Level Security (RLS) policies</name>

  <files>
  supabase/migrations/20260212000002_create_rls_policies.sql
  </files>

  <action>
  Create comprehensive RLS policies for all user-facing tables. These policies enforce org-scoped data access based on the user's org_id stored in the profiles table.

  **CRITICAL: Enable RLS on all tables BEFORE enabling Realtime to prevent cross-org data leaks.**

  Create `supabase/migrations/20260212000002_create_rls_policies.sql` with the following structure:

  **Step 1: Create helper function to get current user's org_id**
  ```sql
  -- Helper function: returns current user's org_id from profiles
  CREATE OR REPLACE FUNCTION auth.user_org_id()
  RETURNS UUID
  LANGUAGE SQL
  STABLE
  AS $$
    SELECT org_id FROM public.profiles WHERE id = auth.uid();
  $$;
  ```

  **Step 2: Enable RLS on all tables**
  ```sql
  ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;
  ALTER TABLE ad_accounts ENABLE ROW LEVEL SECURITY;
  ALTER TABLE spend_records ENABLE ROW LEVEL SECURITY;
  ALTER TABLE balance_snapshots ENABLE ROW LEVEL SECURITY;
  ALTER TABLE alert_rules ENABLE ROW LEVEL SECURITY;
  ALTER TABLE alerts ENABLE ROW LEVEL SECURITY;
  ALTER TABLE alert_deliveries ENABLE ROW LEVEL SECURITY;
  ALTER TABLE notification_channels ENABLE ROW LEVEL SECURITY;
  ALTER TABLE pipeline_runs ENABLE ROW LEVEL SECURITY;

  -- organizations table: only admins can see/modify
  ALTER TABLE organizations ENABLE ROW LEVEL SECURITY;
  ```

  **Step 3: Create policies for each table**

  **profiles table policies:**
  - SELECT: Users can see profiles in their org
  - INSERT: Service role only (user creation happens via signup trigger)
  - UPDATE: Users can update their own profile, admins can update all in org
  - DELETE: Service role only

  **ad_accounts table policies:**
  - SELECT: All authenticated users in same org
  - INSERT: admin/manager roles only
  - UPDATE: admin/manager roles only
  - DELETE: admin role only

  **spend_records, balance_snapshots policies:**
  - SELECT: All authenticated users in same org
  - INSERT: Service role only (n8n writes via service key)
  - UPDATE: Service role only
  - DELETE: Service role only (data retention)

  **alert_rules policies:**
  - SELECT: All authenticated users in same org
  - INSERT: admin/manager roles only
  - UPDATE: admin/manager roles only
  - DELETE: admin/manager roles only

  **alerts, alert_deliveries policies:**
  - SELECT: All authenticated users in same org
  - INSERT: Service role only (created by Edge Functions)
  - UPDATE: authenticated users (for acknowledgment)
  - DELETE: Service role only

  **notification_channels policies:**
  - SELECT: All authenticated users in same org
  - INSERT: admin role only
  - UPDATE: admin role only
  - DELETE: admin role only

  **pipeline_runs policies:**
  - SELECT: All authenticated users in same org
  - INSERT: Service role only
  - UPDATE: Service role only
  - DELETE: Service role only

  **organizations policies:**
  - SELECT: Users can see their own org only
  - INSERT/UPDATE/DELETE: Service role only

  **Example policy syntax:**
  ```sql
  -- ad_accounts SELECT policy
  CREATE POLICY "Users can view ad_accounts in their org"
    ON ad_accounts FOR SELECT
    TO authenticated
    USING (org_id = auth.user_org_id());

  -- ad_accounts INSERT policy (admin/manager only)
  CREATE POLICY "Admins and managers can create ad_accounts"
    ON ad_accounts FOR INSERT
    TO authenticated
    WITH CHECK (
      org_id = auth.user_org_id()
      AND (
        SELECT role FROM profiles WHERE id = auth.uid()
      ) IN ('admin', 'manager')
    );
  ```

  **IMPORTANT:**
  - Use `auth.user_org_id()` helper function consistently
  - Service role bypasses RLS automatically
  - Anon key + JWT token enforces RLS
  - Test policies by connecting as different users
  </action>

  <verify>
  ```bash
  # Verify RLS migration exists
  ls supabase/migrations/20260212000002_create_rls_policies.sql

  # Check that RLS is enabled on all tables
  grep -c "ENABLE ROW LEVEL SECURITY" supabase/migrations/20260212000002_create_rls_policies.sql
  # Should output: 10+

  # Check that policies are created
  grep -c "CREATE POLICY" supabase/migrations/20260212000002_create_rls_policies.sql
  # Should output: 30+ (multiple policies per table)

  # Verify helper function exists
  grep "auth.user_org_id()" supabase/migrations/20260212000002_create_rls_policies.sql
  ```
  </verify>

  <done>
  - RLS enabled on all 10+ user-facing tables
  - Helper function auth.user_org_id() created
  - Policies enforce org-scoped access for authenticated users
  - Service role bypasses RLS (for n8n and Edge Functions)
  - Role-based policies (admin/manager/viewer) implemented
  </done>
</task>

<task type="auto">
  <name>Task 3: Create database triggers for denormalized fields</name>

  <files>
  supabase/migrations/20260212000003_create_triggers.sql
  </files>

  <action>
  Create database triggers that automatically update the denormalized current_* fields on ad_accounts when new time-series data arrives. This ensures fast dashboard reads without complex aggregations.

  Create `supabase/migrations/20260212000003_create_triggers.sql`:

  **Trigger 1: Update current_balance and last_synced_at on balance_snapshots INSERT**
  ```sql
  CREATE OR REPLACE FUNCTION update_ad_account_balance()
  RETURNS TRIGGER
  LANGUAGE plpgsql
  AS $$
  BEGIN
    UPDATE ad_accounts
    SET
      current_balance = NEW.balance,
      last_synced_at = NEW.captured_at,
      updated_at = now()
    WHERE id = NEW.ad_account_id;

    RETURN NEW;
  END;
  $$;

  CREATE TRIGGER on_balance_snapshot_insert
    AFTER INSERT ON balance_snapshots
    FOR EACH ROW
    EXECUTE FUNCTION update_ad_account_balance();
  ```

  **Trigger 2: Update current_daily_spend and current_mtd_spend on spend_records INSERT/UPDATE**
  ```sql
  CREATE OR REPLACE FUNCTION update_ad_account_spend()
  RETURNS TRIGGER
  LANGUAGE plpgsql
  AS $$
  BEGIN
    UPDATE ad_accounts
    SET
      current_daily_spend = NEW.daily_spend,
      current_mtd_spend = NEW.mtd_spend,
      last_synced_at = now(),
      updated_at = now()
    WHERE id = NEW.ad_account_id;

    RETURN NEW;
  END;
  $$;

  CREATE TRIGGER on_spend_record_upsert
    AFTER INSERT OR UPDATE ON spend_records
    FOR EACH ROW
    EXECUTE FUNCTION update_ad_account_spend();
  ```

  **Trigger 3: Auto-update updated_at timestamp on all tables with updated_at column**
  ```sql
  CREATE OR REPLACE FUNCTION update_updated_at_column()
  RETURNS TRIGGER
  LANGUAGE plpgsql
  AS $$
  BEGIN
    NEW.updated_at = now();
    RETURN NEW;
  END;
  $$;

  -- Apply to all tables with updated_at column
  CREATE TRIGGER set_updated_at BEFORE UPDATE ON organizations
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

  CREATE TRIGGER set_updated_at BEFORE UPDATE ON profiles
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

  CREATE TRIGGER set_updated_at BEFORE UPDATE ON ad_accounts
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

  CREATE TRIGGER set_updated_at BEFORE UPDATE ON spend_records
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

  CREATE TRIGGER set_updated_at BEFORE UPDATE ON alert_rules
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

  CREATE TRIGGER set_updated_at BEFORE UPDATE ON notification_channels
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
  ```

  **IMPORTANT:**
  - Triggers run synchronously on INSERT/UPDATE
  - Keep trigger functions lightweight (single UPDATE statement)
  - Test trigger behavior with sample data
  - Do NOT add triggers for alert evaluation yet (Phase 4)
  </action>

  <verify>
  ```bash
  # Verify trigger migration exists
  ls supabase/migrations/20260212000003_create_triggers.sql

  # Check trigger functions created
  grep -c "CREATE OR REPLACE FUNCTION" supabase/migrations/20260212000003_create_triggers.sql
  # Should output: 3

  # Check triggers created
  grep -c "CREATE TRIGGER" supabase/migrations/20260212000003_create_triggers.sql
  # Should output: 8+ (multiple tables)

  # Test trigger by inserting test data
  # (This will be done in verification step after migration runs)
  ```
  </verify>

  <done>
  - Trigger functions created for balance and spend updates
  - Triggers attached to balance_snapshots and spend_records tables
  - Auto-updating updated_at trigger applied to all relevant tables
  - Triggers maintain denormalized current_* fields on ad_accounts
  </done>
</task>

<task type="auto">
  <name>Task 4: Seed initial data and configure Supabase Auth</name>

  <files>
  supabase/migrations/20260212000004_seed_initial_data.sql
  </files>

  <action>
  Seed the database with initial reference data and configure Supabase Auth with custom claims for role-based access.

  Create `supabase/migrations/20260212000004_seed_initial_data.sql`:

  **Step 1: Seed organizations table**
  ```sql
  -- Insert Targetspro organization
  INSERT INTO organizations (id, name, slug, timezone)
  VALUES (
    '00000000-0000-0000-0000-000000000001', -- Fixed UUID for single-tenant
    'Targetspro',
    'targetspro',
    'Africa/Cairo'
  )
  ON CONFLICT (slug) DO NOTHING;
  ```

  **Step 2: Seed platforms table**
  ```sql
  INSERT INTO platforms (id, display_name, api_version, is_active)
  VALUES
    ('facebook', 'Facebook Ads', 'v23.0', true),
    ('tiktok', 'TikTok Ads', 'v1.3', true)
  ON CONFLICT (id) DO UPDATE SET
    api_version = EXCLUDED.api_version,
    is_active = EXCLUDED.is_active;
  ```

  **Step 3: Create trigger to auto-create profile on user signup**
  ```sql
  -- Automatically create profile when user signs up
  CREATE OR REPLACE FUNCTION handle_new_user()
  RETURNS TRIGGER
  LANGUAGE plpgsql
  SECURITY DEFINER
  AS $$
  BEGIN
    INSERT INTO public.profiles (id, org_id, full_name, role)
    VALUES (
      NEW.id,
      '00000000-0000-0000-0000-000000000001', -- Default org
      COALESCE(NEW.raw_user_meta_data->>'full_name', NEW.email),
      COALESCE(NEW.raw_user_meta_data->>'role', 'viewer')
    );
    RETURN NEW;
  END;
  $$;

  CREATE TRIGGER on_auth_user_created
    AFTER INSERT ON auth.users
    FOR EACH ROW
    EXECUTE FUNCTION handle_new_user();
  ```

  **Step 4: Create function to update user role in JWT custom claims**
  ```sql
  -- Update JWT custom claims when profile role changes
  -- This makes the role available in auth.jwt() for RLS policies
  CREATE OR REPLACE FUNCTION update_user_role_claim()
  RETURNS TRIGGER
  LANGUAGE plpgsql
  SECURITY DEFINER
  AS $$
  BEGIN
    -- Update raw_app_meta_data in auth.users
    UPDATE auth.users
    SET raw_app_meta_data =
      COALESCE(raw_app_meta_data, '{}'::jsonb) ||
      jsonb_build_object('role', NEW.role, 'org_id', NEW.org_id)
    WHERE id = NEW.id;

    RETURN NEW;
  END;
  $$;

  CREATE TRIGGER on_profile_role_change
    AFTER INSERT OR UPDATE OF role, org_id ON profiles
    FOR EACH ROW
    EXECUTE FUNCTION update_user_role_claim();
  ```

  **Step 5: Create default notification channels**
  ```sql
  -- Create default email channel for Targetspro org
  INSERT INTO notification_channels (org_id, channel_type, name, config, min_severity, is_enabled)
  VALUES (
    '00000000-0000-0000-0000-000000000001',
    'email',
    'Default Email Alerts',
    '{"recipients": ["info@targetspro.com"]}'::jsonb,
    'warning',
    true
  )
  ON CONFLICT DO NOTHING;
  ```

  **IMPORTANT:**
  - Use fixed UUID for Targetspro org (single-tenant start)
  - Role is stored in both profiles.role and auth.users.raw_app_meta_data
  - JWT tokens include role and org_id in custom claims
  - Profile is auto-created on signup (no manual step needed)
  </action>

  <verify>
  ```bash
  # Verify seed migration exists
  ls supabase/migrations/20260212000004_seed_initial_data.sql

  # Run all migrations locally
  supabase db reset

  # Verify organizations seeded
  supabase db query "SELECT * FROM organizations;"
  # Should show Targetspro org

  # Verify platforms seeded
  supabase db query "SELECT * FROM platforms;"
  # Should show facebook and tiktok

  # Verify triggers created
  supabase db query "SELECT trigger_name FROM information_schema.triggers WHERE trigger_schema = 'public';"
  # Should include: on_auth_user_created, on_profile_role_change
  ```
  </verify>

  <done>
  - Targetspro organization seeded with fixed UUID
  - Facebook and TikTok platforms seeded
  - Auto-profile-creation trigger on auth.users
  - Role sync trigger updates JWT custom claims
  - Default email notification channel created
  - All migrations run successfully via supabase db reset
  </done>
</task>

<task type="auto">
  <name>Task 5: Create data migration script from legacy tables</name>

  <files>
  scripts/migrate_legacy_data.ts
  package.json
  .env.local.example
  </files>

  <action>
  Create a TypeScript script that safely migrates data from the 7 existing legacy tables into the new normalized schema. This script must preserve all historical data and handle platform-specific differences (Facebook vs TikTok).

  **Step 1: Install dependencies**
  ```bash
  npm install @supabase/supabase-js dotenv tsx
  npm install -D @types/node
  ```

  **Step 2: Create .env.local.example**
  ```
  SUPABASE_URL=https://your-project.supabase.co
  SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
  ```

  **Step 3: Create scripts/migrate_legacy_data.ts**

  The script should:

  **1. Connect to Supabase using service role key (bypasses RLS)**
  ```typescript
  import { createClient } from '@supabase/supabase-js';
  import * as dotenv from 'dotenv';

  dotenv.config({ path: '.env.local' });

  const supabase = createClient(
    process.env.SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY!
  );

  const TARGETSPRO_ORG_ID = '00000000-0000-0000-0000-000000000001';
  ```

  **2. Get platform IDs**
  ```typescript
  const FACEBOOK_PLATFORM_ID = 'facebook';
  const TIKTOK_PLATFORM_ID = 'tiktok';
  ```

  **3. Define legacy table names**
  ```typescript
  const LEGACY_FACEBOOK_TABLES = [
    'Facebook Data Pull —Main accounts',
    'Facebook Data Pull —Pasant',
    'Facebook Data Pull —aligomarketing',
    'Facebook Data Pull —Xlerate'
  ];

  const LEGACY_TIKTOK_TABLES = [
    'Tiktok accounts',
    'tiktok2'
  ];
  ```

  **4. Migrate Facebook accounts**
  For each Facebook table:
  - Read all rows
  - Map columns: "Account ID" -> platform_account_id, "Account name" -> account_name, "Status" -> status (lowercase), "Available funds" -> current_balance, "Daily spending" -> current_daily_spend, "Client Name" -> metadata.client_name
  - Extract business_manager from table name (Main, Pasant, aligomarketing, Xlerate)
  - UPSERT into ad_accounts (conflict on org_id, platform_id, platform_account_id)
  - Create initial balance_snapshots entry with "Available funds" value
  - Create initial spend_records entry with "Daily spending" value for today's date

  **5. Migrate TikTok accounts**
  For each TikTok table:
  - Read all rows
  - Map columns: "Advertiser_id" -> platform_account_id, "Advertiser name" -> account_name, "Status" -> status (lowercase), "Available funds" -> current_balance, "Daily spending" -> current_daily_spend, "BC-ID" -> metadata.bc_id
  - business_manager is NULL for TikTok
  - UPSERT into ad_accounts
  - Create initial balance_snapshots and spend_records

  **6. Handle currency conversion**
  - Facebook stores balance in micro-units (divide by 100 for EGP)
  - TikTok stores balance in currency units (no conversion)
  - Parse "Available funds" text field to extract numeric value

  **7. Error handling and logging**
  - Log each table being processed
  - Log row counts (total, succeeded, failed)
  - Wrap in try-catch per table (one table failure doesn't stop others)
  - Create migration report at end

  **8. Dry-run mode**
  - Accept --dry-run flag to preview without writing
  - Log what would be inserted

  **CRITICAL NOTES:**
  - DO NOT modify or delete legacy tables (they must keep running)
  - Use UPSERT (ON CONFLICT DO UPDATE) to make script idempotent
  - Parse "Available funds" string carefully (may include currency symbols)
  - Handle missing/null values gracefully
  - Preserve original data in ad_accounts.metadata JSONB for debugging
  - Use transactions where possible for data consistency
  </action>

  <verify>
  ```bash
  # Verify migration script exists
  ls scripts/migrate_legacy_data.ts

  # Run in dry-run mode first
  npm run migrate:dry-run
  # (Add script to package.json: "migrate:dry-run": "tsx scripts/migrate_legacy_data.ts --dry-run")

  # Check for errors in dry-run output
  # Should show: "Would migrate X accounts from Y tables"

  # Run actual migration
  npm run migrate
  # (Add script to package.json: "migrate": "tsx scripts/migrate_legacy_data.ts")

  # Verify migrated data
  supabase db query "SELECT COUNT(*) FROM ad_accounts;"
  supabase db query "SELECT COUNT(*) FROM balance_snapshots;"
  supabase db query "SELECT COUNT(*) FROM spend_records;"

  # Spot-check: compare one account's balance in old vs new table
  supabase db query "SELECT platform_account_id, current_balance FROM ad_accounts WHERE platform_id = 'facebook' LIMIT 5;"
  # Manually verify against legacy table
  ```
  </verify>

  <done>
  - Migration script created with support for all 7 legacy tables
  - Facebook and TikTok account mappings implemented
  - Currency conversion handled correctly (Facebook micro-units)
  - UPSERT logic makes script idempotent (re-runnable)
  - Dry-run mode works and shows migration preview
  - Actual migration completes without data loss
  - Migrated row counts match legacy table row counts
  - Legacy tables remain untouched and operational
  </done>
</task>

<task type="auto">
  <name>Task 6: Generate TypeScript types and setup type-safe database access</name>

  <files>
  lib/database.types.ts
  lib/supabase.ts
  tsconfig.json
  package.json
  </files>

  <action>
  Generate TypeScript types from the new Supabase schema and create a type-safe database client for the Next.js application.

  **Step 1: Generate types from Supabase schema**
  ```bash
  # Install Supabase CLI if not already installed
  npm install -g supabase

  # Generate TypeScript types
  supabase gen types typescript --local > lib/database.types.ts

  # Or for remote project:
  # supabase gen types typescript --project-id YOUR_PROJECT_ID > lib/database.types.ts
  ```

  **Step 2: Create type-safe Supabase client (lib/supabase.ts)**
  ```typescript
  import { createClient } from '@supabase/supabase-js';
  import type { Database } from './database.types';

  const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!;
  const supabaseAnonKey = process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!;

  export const supabase = createClient<Database>(supabaseUrl, supabaseAnonKey);

  // Type exports for convenience
  export type Tables<T extends keyof Database['public']['Tables']> =
    Database['public']['Tables'][T]['Row'];

  export type Enums<T extends keyof Database['public']['Enums']> =
    Database['public']['Enums'][T];

  // Specific type exports
  export type Organization = Tables<'organizations'>;
  export type Profile = Tables<'profiles'>;
  export type AdAccount = Tables<'ad_accounts'>;
  export type SpendRecord = Tables<'spend_records'>;
  export type BalanceSnapshot = Tables<'balance_snapshots'>;
  export type AlertRule = Tables<'alert_rules'>;
  export type Alert = Tables<'alerts'>;
  export type AlertDelivery = Tables<'alert_deliveries'>;
  export type NotificationChannel = Tables<'notification_channels'>;
  export type PipelineRun = Tables<'pipeline_runs'>;
  ```

  **Step 3: Update tsconfig.json**
  Ensure strict mode is enabled and paths are configured:
  ```json
  {
    "compilerOptions": {
      "strict": true,
      "strictNullChecks": true,
      "noImplicitAny": true,
      "paths": {
        "@/*": ["./*"],
        "@/lib/*": ["./lib/*"]
      }
    }
  }
  ```

  **Step 4: Add type generation script to package.json**
  ```json
  {
    "scripts": {
      "db:types": "supabase gen types typescript --local > lib/database.types.ts",
      "db:reset": "supabase db reset",
      "migrate": "tsx scripts/migrate_legacy_data.ts",
      "migrate:dry-run": "tsx scripts/migrate_legacy_data.ts --dry-run"
    }
  }
  ```

  **Step 5: Create example query to verify types**
  Create a simple test file to verify type safety:
  ```typescript
  // test/type-check.ts (not committed)
  import { supabase, type AdAccount } from '../lib/supabase';

  async function testTypes() {
    const { data, error } = await supabase
      .from('ad_accounts')
      .select('*')
      .eq('platform_id', 'facebook');

    if (data) {
      // TypeScript should know data is AdAccount[]
      const accounts: AdAccount[] = data;
      console.log('Type-safe query works:', accounts.length);
    }
  }

  testTypes();
  ```

  **IMPORTANT:**
  - Re-run `npm run db:types` after any schema changes
  - Types are auto-generated, never manually edit database.types.ts
  - Use the typed client for all database queries
  - Test type safety by attempting invalid queries (should fail at compile time)
  </action>

  <verify>
  ```bash
  # Verify types file generated
  ls lib/database.types.ts

  # Check file size (should be substantial)
  wc -l lib/database.types.ts
  # Should be 500+ lines

  # Verify exports
  grep "export type Database" lib/database.types.ts

  # Verify client file exists
  ls lib/supabase.ts

  # Run TypeScript compiler to check for type errors
  npx tsc --noEmit
  # Should output: no errors

  # Test type-safe query (optional)
  # npm run tsx test/type-check.ts
  ```
  </verify>

  <done>
  - TypeScript types generated from Supabase schema
  - lib/database.types.ts contains all table and enum definitions
  - lib/supabase.ts exports type-safe client
  - Convenience type exports created (Organization, AdAccount, etc.)
  - package.json scripts added for type regeneration
  - TypeScript compiler validates types without errors
  - Type safety verified with test query
  </done>
</task>

## Verification

**Overall Phase Verification:**

Run the following checks to confirm Phase 1 is complete:

```bash
# 1. Check all migration files exist
ls supabase/migrations/
# Should show:
# - 20260212000001_create_core_schema.sql
# - 20260212000002_create_rls_policies.sql
# - 20260212000003_create_triggers.sql
# - 20260212000004_seed_initial_data.sql

# 2. Apply all migrations
supabase db reset

# 3. Verify table count
supabase db query "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';"
# Should show 11 tables (10 + platforms reference table)

# 4. Verify RLS enabled
supabase db query "SELECT tablename, rowsecurity FROM pg_tables WHERE schemaname = 'public';"
# All tables should show rowsecurity = true

# 5. Verify triggers exist
supabase db query "SELECT trigger_name, event_object_table FROM information_schema.triggers WHERE trigger_schema = 'public' ORDER BY trigger_name;"
# Should show all triggers including on_balance_snapshot_insert, on_spend_record_upsert

# 6. Run migration script (dry-run)
npm run migrate:dry-run
# Should show accounts to be migrated without errors

# 7. Run actual migration
npm run migrate
# Should complete without errors

# 8. Verify migrated data
supabase db query "SELECT COUNT(*) FROM ad_accounts;"
# Should match total rows from 7 legacy tables

supabase db query "SELECT platform_id, COUNT(*) FROM ad_accounts GROUP BY platform_id;"
# Should show counts for 'facebook' and 'tiktok'

# 9. Verify triggers work
supabase db query "
  INSERT INTO balance_snapshots (org_id, ad_account_id, balance, currency)
  SELECT org_id, id, 10000.00, 'EGP'
  FROM ad_accounts LIMIT 1;
"
supabase db query "
  SELECT id, current_balance, last_synced_at FROM ad_accounts WHERE current_balance = 10000.00;
"
# Should show the updated ad_account with current_balance = 10000.00

# 10. Verify TypeScript types
npx tsc --noEmit
# Should have no type errors

# 11. Test RLS policies (connect as authenticated user)
# This will be tested in Phase 3 when dashboard is built
```

**Legacy System Verification:**
- [ ] All 7 legacy tables still exist and untouched
- [ ] Legacy triggers still fire (check `notify_webhook_on_update` function exists)
- [ ] n8n workflows can still write to legacy tables
- [ ] No data deleted or modified in legacy tables

**Data Integrity Verification:**
- [ ] Row counts match between legacy and new tables
- [ ] Spot-check 5 accounts: balances match between old and new schema
- [ ] No NULL values in required fields (account_name, platform_account_id)
- [ ] All platform_account_ids are unique per platform

**Performance Verification:**
- [ ] Query `SELECT * FROM ad_accounts WHERE org_id = 'xxx'` returns in <100ms
- [ ] Query `SELECT * FROM spend_records WHERE ad_account_id = 'xxx' ORDER BY date DESC LIMIT 30` returns in <50ms
- [ ] Trigger execution time: INSERT into balance_snapshots completes in <10ms

## Success Criteria

Phase 1 is complete when:

- [x] All 10 core tables created with proper constraints and indexes
- [x] RLS policies enabled on all tables, enforcing org-scoped access
- [x] Database triggers update denormalized current_* fields automatically
- [x] Supabase Auth configured with role-based access (admin, manager, viewer)
- [x] Legacy data migrated from 7 old tables to new normalized schema
- [x] Zero data loss during migration (row counts match)
- [x] Legacy tables remain operational (no disruption to existing n8n workflows)
- [x] TypeScript types generated and type-safe client available
- [x] All migrations run successfully via `supabase db reset`
- [x] Triggers verified to work correctly (test INSERT updates ad_accounts)
- [x] Migration script is idempotent (can be re-run safely)

**Acceptance Criteria:**
- Database schema matches requirements R2.1-R2.8 exactly
- RLS policies pass security test (users cannot see other orgs' data)
- Triggers fire correctly on INSERT (verified with test data)
- Migration script completes without errors
- TypeScript compiler validates types without errors
- All legacy data preserved and accessible in new schema

## Output

After completion, create:

`.planning/phases/01-database-foundation-schema-migration/01-01-SUMMARY.md`

Include:
- Tables created (list all 10)
- RLS policies implemented (count per table)
- Triggers created (list all 3 trigger functions)
- Migration results (rows migrated per table)
- Any issues encountered and resolutions
- Performance notes (query times, trigger execution times)
- Next steps: Phase 2 can now begin (n8n writes to new tables)
