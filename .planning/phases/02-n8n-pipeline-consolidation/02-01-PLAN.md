---
phase: 02-n8n-pipeline-consolidation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - n8n-workflows/facebook-ingestion.json
  - n8n-workflows/README.md
autonomous: true

must_haves:
  truths:
    - "Facebook Ingestion workflow reads active Facebook ad_accounts from Supabase"
    - "Facebook Ingestion workflow calls Graph API v23.0 using batch requests (up to 50 per batch)"
    - "Facebook balance values are divided by 100 before writing to Supabase"
    - "All date calculations use Luxon setZone('Africa/Cairo'), never manual UTC offsets"
    - "Workflow creates a pipeline_run row at start and finalizes it at end with correct status"
    - "Workflow upserts spend_records with UNIQUE(ad_account_id, date) conflict handling"
    - "Workflow inserts balance_snapshots with pipeline_run_id reference"
    - "Workflow writes to legacy tables for dual-write validation (R3.6)"
    - "Workflow does NOT write to Google Sheets (R3.4)"
    - "Per-account errors are caught and logged without stopping the entire batch"
  artifacts:
    - path: "n8n-workflows/facebook-ingestion.json"
      provides: "Complete n8n workflow JSON for Facebook data ingestion"
      contains: "Execute Sub-workflow Trigger"
    - path: "n8n-workflows/README.md"
      provides: "Import instructions, credential setup, and workflow structure documentation"
  key_links:
    - from: "facebook-ingestion.json (Execute Sub-workflow Trigger)"
      to: "Controller workflow"
      via: "Execute Sub-workflow node call with typed inputs (org_id, pipeline_name)"
      pattern: "executeSubWorkflowTrigger"
    - from: "facebook-ingestion.json (HTTP Request nodes)"
      to: "Facebook Graph API v23.0"
      via: "POST batch requests to https://graph.facebook.com/v23.0/"
      pattern: "graph.facebook.com/v23.0"
    - from: "facebook-ingestion.json (Supabase nodes)"
      to: "pipeline_runs, spend_records, balance_snapshots, ad_accounts tables"
      via: "n8n Supabase node upsert/insert operations"
      pattern: "supabase"
---

<objective>
Build the Facebook Ingestion n8n workflow that replaces 4 separate per-BM Facebook workflows with a single parameterized workflow handling all 4 Business Managers through one API connection and batch requests.

Purpose: This is the most complex ingestion workflow. It introduces the batch API pattern (reducing ~90 individual calls to ~9), correct balance conversion (divide by 100), Cairo timezone handling via Luxon, pipeline_run lifecycle logging, and dual-write to both new normalized tables and old legacy tables.

Output: Importable n8n workflow JSON file + documentation for credential setup and import.
</objective>

<execution_context>
@C:/Users/hossa/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/hossa/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-n8n-pipeline-consolidation/02-RESEARCH.md
@supabase/migrations/20260212000001_create_core_schema.sql
@supabase/migrations/20260212000003_create_triggers.sql
@supabase/migrations/20260212000004_seed_initial_data.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Facebook Ingestion workflow JSON</name>
  <files>n8n-workflows/facebook-ingestion.json</files>
  <action>
Create the directory `n8n-workflows/` and build a complete n8n workflow JSON file for the Facebook Ingestion workflow. The workflow structure must follow this exact node chain:

**Node 1: Execute Sub-workflow Trigger**
- Input data mode: "Define using fields below"
- Fields: `org_id` (string, default '00000000-0000-0000-0000-000000000001'), `pipeline_name` (string, default 'facebook_ingestion')
- This is the entry point -- the Controller workflow will call this

**Node 2: Compute Cairo Dates (Code node)**
- Use Luxon: `const cairo = DateTime.now().setZone('Africa/Cairo');`
- Compute: `today`, `yesterday`, `startOfMonth` (all yyyy-MM-dd format)
- Compute: `cairoTimestamp` (ISO string for pipeline logging)
- Do NOT use manual UTC+2 or UTC+3 offsets anywhere

**Node 3: Create Pipeline Run (Supabase INSERT)**
- Table: `pipeline_runs`
- Fields: org_id (from trigger input), pipeline_name (from trigger input), status='running', started_at=now(), accounts_processed=0, accounts_failed=0, metadata={ triggered_by: 'controller', n8n_execution_id: $execution.id }
- Capture the returned `id` as pipeline_run_id for downstream nodes

**Node 4: Fetch Active Facebook Accounts (Supabase SELECT)**
- Table: `ad_accounts`
- Filters: org_id = input org_id, platform_id = 'facebook', status = 'active'
- Select all columns (need platform_account_id, id, business_manager, metadata)

**Node 5: Build Account Info Batch Request (Code node)**
- Take the list of ad_accounts and create Facebook Batch API request bodies
- For each account: `{ method: 'GET', relative_url: '{platform_account_id}?fields=name,balance,amount_spent,account_status,funding_source_details{display_string}' }`
- Split into chunks of 50 (Facebook hard limit per batch)
- Output: one item per chunk, each with a `batch` field containing JSON.stringify'd array

**Node 6: Execute Account Info Batch (HTTP Request node)**
- Method: POST
- URL: `https://graph.facebook.com/v23.0/`
- Authentication: Use predefined credential (Facebook Graph API type, credential ID to be configured)
- Body Content Type: Form URL Encoded
- Body: `batch` = expression from previous Code node, `include_headers` = `false`
- Note: access_token is sent via the Facebook Graph API credential automatically

**Node 7: Build Daily Insights Batch Request (Code node)**
- For each account: `{ method: 'GET', relative_url: '{platform_account_id}/insights?fields=spend&time_range={"since":"{yesterday}","until":"{yesterday}"}&level=account' }`
- Same 50-per-chunk splitting

**Node 8: Execute Daily Insights Batch (HTTP Request node)**
- Same pattern as Node 6, different batch body

**Node 9: Build MTD Insights Batch Request (Code node)**
- For each account: `{ method: 'GET', relative_url: '{platform_account_id}/insights?fields=spend&time_range={"since":"{startOfMonth}","until":"{today}"}&level=account' }`
- Same 50-per-chunk splitting

**Node 10: Execute MTD Insights Batch (HTTP Request node)**
- Same pattern as Node 6, different batch body

**Node 11: Parse and Normalize All Batch Responses (Code node)**
This is the central transformation node. For each account:
- Parse batch response bodies: `JSON.parse(item.body)` for each response item
- Match account info, daily insights, and MTD insights by platform_account_id
- **CRITICAL: Divide balance by 100** (Facebook returns micro-units for EGP)
- **CRITICAL: Divide amount_spent by 100** (same micro-unit conversion)
- **CRITICAL: Divide daily spend from insights by 100**
- Map account_status: 1='active', 2='disabled', 3='unsettled', 7='pending_risk_review', 8='pending_settlement', 9='in_grace_period', 100='pending_closure', 101='closed', 201='any_active', 202='any_closed'
- Normalize to lowercase status string
- Use Cairo date from Node 2 for the spend_records date field
- Attach pipeline_run_id from Node 3
- Build error array for any individual batch items with code !== 200
- Output: array of normalized account objects + error array

**Node 12: Split Results - Success vs Error (IF node)**
- Route successful results to Supabase write nodes
- Route errors to error logging path

**Node 13: Upsert spend_records (Supabase UPSERT)**
- Table: `spend_records`
- Conflict columns: `ad_account_id,date`
- Fields: org_id, ad_account_id (the UUID from ad_accounts table, NOT platform_account_id), date (Cairo date), daily_spend, mtd_spend, currency='EGP', raw_data (original API response as JSONB), pipeline_run_id

**Node 14: Insert balance_snapshots (Supabase INSERT)**
- Table: `balance_snapshots`
- Fields: org_id, ad_account_id (UUID), balance (divided by 100), available_funds (funding_source display string), currency='EGP', captured_at (Cairo ISO timestamp), pipeline_run_id

**Node 15: Update Legacy Tables - Dual Write (Supabase UPSERT)**
- For the dual-write validation period (R3.6), update the old per-BM tables
- This node writes to the legacy table structure matching the old workflow output
- Use the business_manager field to determine which legacy table to target
- Write: Account ID, Account name, Balance (converted), Daily spending (converted), Total spent (MTD converted), Status, Date
- Note: These legacy table names will need to be configured per the existing n8n instance table setup

**Node 16: Aggregate Results (Code node)**
- Count successful vs failed accounts
- Build error_log JSONB object mapping failed platform_account_ids to their error messages
- Determine final status: 'success' (0 failures), 'partial' (some failures), 'failed' (all failures)

**Node 17: Finalize Pipeline Run (Supabase UPDATE)**
- Table: `pipeline_runs`
- Filter: id = pipeline_run_id from Node 3
- Fields: status (from Node 16), completed_at=now(), accounts_processed, accounts_failed, error_log

**Error handling approach:**
- Use n8n's "Continue on Error" setting on HTTP Request nodes (Nodes 6, 8, 10) so batch failures don't crash the whole workflow
- The Code node (Node 11) handles per-account errors within the batch response
- If the entire workflow fails unexpectedly, the Controller's error handler will catch it and mark the pipeline_run as 'failed'
- Do NOT use `continueRegularOutput` (the old anti-pattern); use proper error output branches

**Workflow settings:**
- Set Error Workflow to the Controller's error handler (configurable after import)
- Set timezone to Africa/Cairo in workflow settings
- Retry on fail: disabled (the Controller handles retry scheduling)

**Important constraints:**
- All API calls MUST use v23.0 (R3.2): `https://graph.facebook.com/v23.0/`
- Zero Google Sheets nodes in this workflow (R3.4)
- Credential must be referenced by n8n credential ID, not hardcoded tokens
- The workflow JSON must be valid n8n format importable via the n8n UI or API
  </action>
  <verify>
1. Validate the JSON is syntactically correct: `node -e "const w = require('./n8n-workflows/facebook-ingestion.json'); console.log('Nodes:', w.nodes?.length || Object.keys(w).length)"`
2. Verify no Google Sheets references: search the JSON for "googleSheets" or "Google Sheets" -- must find zero
3. Verify Facebook API v23.0: search the JSON for "graph.facebook.com" -- all instances must include "v23.0"
4. Verify balance division by 100: search Code node contents for "/ 100" or "/100"
5. Verify Luxon timezone usage: search for "setZone" and "Africa/Cairo" -- must be present
6. Verify no manual UTC offset: search for "+2" or "+3" in Code node context -- must not find UTC offset arithmetic
7. Verify pipeline_runs lifecycle: search for "pipeline_runs" -- must appear in both INSERT and UPDATE contexts
  </verify>
  <done>
A valid n8n workflow JSON file exists at n8n-workflows/facebook-ingestion.json that:
- Has an Execute Sub-workflow Trigger as entry point with typed inputs (org_id, pipeline_name)
- Uses Facebook Batch API via HTTP Request nodes (not individual calls per account)
- Divides all Facebook monetary values by 100 before writing
- Uses Luxon setZone('Africa/Cairo') for all date calculations
- Creates pipeline_run at start, finalizes at end with correct status
- Upserts spend_records with conflict on (ad_account_id, date)
- Inserts balance_snapshots with pipeline_run_id
- Writes to legacy tables for dual-write validation
- Contains zero Google Sheets nodes
- All Facebook API URLs target v23.0
  </done>
</task>

<task type="auto">
  <name>Task 2: Create workflow documentation and credential setup guide</name>
  <files>n8n-workflows/README.md</files>
  <action>
Create a README.md in the n8n-workflows/ directory documenting:

**1. Overview**
- Purpose: 4 consolidated workflows replacing 8 legacy workflows
- Architecture diagram (text-based): Controller -> Facebook Ingestion, TikTok 1, TikTok 2
- Data flow: API -> n8n -> Supabase (new tables) + Legacy tables (dual-write)

**2. Import Instructions**
- How to import each workflow JSON into n8n (via UI: Settings > Import Workflow; via API: POST /api/v1/workflows)
- Order of import: Ingestion workflows first, then Controller (which references them by ID)
- After import, update the Controller's Execute Sub-workflow nodes to point to the correct imported workflow IDs

**3. Credential Requirements**
List all credentials that must be configured in n8n before workflows will function:

| Credential | Type | Where to Get | Used By |
|------------|------|-------------|---------|
| Facebook Graph API | Facebook Graph API | Facebook Business Manager > System User > Generate Token (must have access to all 4 BMs) | facebook-ingestion.json |
| Supabase Service Role | Header Auth (apikey header) or Supabase credential | Supabase Dashboard > Settings > API > service_role key | All workflows |
| TikTok Token Group 1 | Header Auth (Access-Token header) | TikTok Business API developer portal | tiktok-ingestion-1.json |
| TikTok Token Group 2 | Header Auth (Access-Token header) | TikTok Business API developer portal | tiktok-ingestion-2.json |

**4. Configuration After Import**
- Set each workflow's Error Workflow to the Controller's error handler
- Update the Controller's Execute Sub-workflow nodes with the imported workflow IDs
- Verify Supabase credential is set as the default for all Supabase nodes
- Set workflow timezone to Africa/Cairo in each workflow's settings

**5. Legacy Table Mapping**
Document the dual-write target tables (the old per-BM tables) and explain:
- Which legacy table corresponds to which business_manager value
- That dual-write is temporary for validation
- Criteria for disabling dual-write (7 consecutive days of zero discrepancies)

**6. Validation Checklist**
- [ ] Import all 4 workflows
- [ ] Configure all credentials
- [ ] Run Controller manually once
- [ ] Compare new table data with old table data for same time period
- [ ] Verify pipeline_runs shows correct status
- [ ] Verify zero Google Sheets writes in n8n execution logs
- [ ] Disable old 8 workflows only after validation passes

**7. Rollback Plan**
- Re-enable old 8 workflows
- New workflows can be disabled without data loss (new tables retain data)
- Legacy table dual-write ensures old system has current data during transition
  </action>
  <verify>
1. File exists at n8n-workflows/README.md
2. Contains credential setup table
3. Contains import order instructions
4. Contains validation checklist
5. Contains rollback plan
  </verify>
  <done>
README.md exists with complete import instructions, credential requirements, configuration steps, validation checklist, and rollback plan -- sufficient for a user to set up all 4 workflows on their n8n instance.
  </done>
</task>

</tasks>

<verification>
- The facebook-ingestion.json workflow is valid JSON and importable into n8n
- All Facebook API calls target v23.0
- Balance conversion divides by 100
- All dates use Luxon setZone('Africa/Cairo')
- Pipeline run lifecycle (running -> success/partial/failed) is complete
- Zero Google Sheets references in the workflow
- Dual-write to legacy tables is present
- README covers all credential and setup requirements
</verification>

<success_criteria>
- facebook-ingestion.json exists as valid n8n workflow JSON
- Workflow has 15-17 nodes following the documented architecture
- All monetary values from Facebook are divided by 100 before Supabase writes
- Pipeline runs are logged with per-account error tracking
- README provides complete setup guide for importing and configuring the workflow
</success_criteria>

<output>
After completion, create `.planning/phases/02-n8n-pipeline-consolidation/02-01-SUMMARY.md`
</output>
